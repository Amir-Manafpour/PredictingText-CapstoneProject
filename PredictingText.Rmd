---
title: "PredictingText"
author: "AM"
date: "5/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(readr)
library(tm)
library(dplyr)
library(hunspell)
library(ggplot2)
library(tokenizers)
library(cld3)
library(data.table)
library(tidyr)
memory.limit(100000)
```

# Data Import

The code section below outlines the data import process of the three text files for this project.

```{r data-import}
# Set directories for files
setwd("C:/Users/amanafpour/Desktop/final/en_US")

# Load the dataset
filenames <- list.files(".") #read filenames from directory
dfread <- NULL
dfread <- data.frame(file = character(), lines = character())
linesnmax <- 50000 #Set maximum for number of lines read of each file
for (filename in filenames) {
  lins <- readr::read_lines(filename, n_max = linesnmax) 
  newdf <- data.frame(file = filename, lines = lins)
  dfread <- rbind(dfread, newdf)
}
dfread$file<-as.factor(dfread$file)

# Randomly select subset of data
nsamp <- 2000
set.seed(34341)
dftibble <- dfread %>% group_by(file) %>% sample_n(size = nsamp)
df <- as.data.frame(dftibble)

# Generate docs corpus from data frame (1 text doc per input txt file)
docs <- NULL
for (file in levels(df$file)){
  subdf <- df[which(df$file == file),]
  doc_str <- paste(subdf$lines, collapse = "\n")
  #Initiate corpus if it doesn't exist and set the ptd's meta data
  tempdoc <- VCorpus(VectorSource(doc_str),
                     readerControl = list(reader=readPlain, language = "en"))
  meta(tempdoc[[1]])$id <- file
  if (is.null(docs)) {docs <- tempdoc}
  else {docs <- c(docs, tempdoc)}
}
docsOrig <- docs
```

It is worth noting, for testing purposes and RAM limitations, the first **`r format(linesnmax, scientific = F)`** lines were read from each input file and then only a small subset (**`r nsamp`** samples per input file) was selected with equal number of samples per input file to generate the corpora for analysis.

# Data Cleaning

The function defined below cleans the corpus. Profanity is removed but only the most explicit swear words. Removal of stopwords is left as an option for the function because we may wan to include them in the prediction model.

```{r data-cleaning-funcs}
# Punctuation to be removed during tokenization, not here, because periods '.' will be
# used to determine sentences in the tokenization section. 
Clean_Corpus <- function(corpus, removeStpwrds = T){
  # Lowercase
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Remove numbers
  corpus <- tm_map(corpus, removeNumbers)
  # Remove explicitly profane words
  profanity <- readLines("http://www.bannedwordlist.com/lists/swearWords.txt")
  corpus <- tm_map(corpus, removeWords, profanity)
  
  # Replace all misspelled words with a \n line break
  # badwrds <- unlist(hunspell(x))
  # corpus <- tm_map(corpus, content_transformer(function (x){
  #   badwrds <- unlist(hunspell(x))
  #   for (wrd in badwrds){
  #     x <- gsub(wrd, "\n", as.String(x))
  #   }
  #   return(x)
  # }))
  
  # Remove extra whitespace BUT maintain \n line breaks
  whitespaceFUN <- content_transformer(function(x) gsub("[ ]+", " ",as.String(x)))
  corpus <- tm_map(corpus, whitespaceFUN)
  # Remove stop words if applicable
  if (removeStpwrds == T) {
    corpus <- tm_map(corpus, removeWords, words = stopwords("en"))
    }
  return(corpus)
}
```

# Tokenization

A custom function ngramTokenizer is defined for n-gram tokenization that is used in the DocumentTermMatrix control options. This custom function ensures that the corpora is first divided into individual sentences before tokenizing into words, in order to avoid tokenizing words from different sentences. The Get_Freq function summarizes all rows in the matrix into one vector and the getDoc_ByID function is defined if we want to extract a row from the corpus to evaluate independently.

```{r tokenization-funcs}
# TODO: convert tm corpus to quanteda corpus and change code as required

# ngram tokenization function
ngramTokenizer <- function(x, n){
    sens <- unlist(lapply(tokenize_lines(as.String(x)), tokenize_sentences), use.names = F)
    ngrams <- unlist(tokenize_ngrams(sens, n=n, lowercase = F))
    ngrams[!is.na(ngrams)]
  }

Get_dtm <- function(corpus, n = 1, rmvPunc = T, spars = 1.0){
  if (n == 1){
    dtm <- DocumentTermMatrix(corpus, control = list(removePunctuation = rmvPunc))
    }
  else {
    dtm <- DocumentTermMatrix(corpus,
                                control = list(removePunctuation = T,
                                               tokenize = function(x) ngramTokenizer(x,n)))
  }
  if (spars < 1.0 & spars > 0.0) dtm <- removeSparseTerms(dtm, spars)
  return(dtm)
}

# Get_Freq summarizes all rows in the document-term matrix into one vector
Get_Freq <- function(dtminput){
  freq <- colSums(as.matrix(dtminput))
  ord <- order(freq, decreasing = T)
  freq <- freq[ord]
  return(freq)
}

# Custom function to return only english terms of freq vector
getEngFreq <- function(freqV) {
  wrongTerms <- hunspell(names(freqV))
  freqV[sapply(wrongTerms, identical, character(0))]
}

# Custom function for getting doc from corpus
getDoc_ByID <- function(corpus, id) corpus[grep(id, unlist(meta(corpus, "id")), fixed = T)]
```

# Exploratory Data Analysis

The document-term matrices and n-grams are further evaluated below for all three documents together and separately. A custom function is defined to plot different frequency vectors.

```{r exploratory-analysis}
# Clean corpus removing stopwords and tokenize
docs <- Clean_Corpus(docs)
freqWords <- Get_Freq(Get_dtm(docs))
freqWords_blogs <- Get_Freq(Get_dtm(getDoc_ByID(docs, "blogs")))
freqWords_news <- Get_Freq(Get_dtm(getDoc_ByID(docs, "news")))
freqWords_twitter <- Get_Freq(Get_dtm(getDoc_ByID(docs, "twitter")))
freqBigram <- Get_Freq(Get_dtm(docs, n = 2))
freqTrigram <- Get_Freq(Get_dtm(docs, n = 3))

# Function for plotting terms vs occurrences
plot_occurrences <- function(freqplot, nwords = 20){
  wf <- data.frame(term = names(freqplot),
                   occurrences = freqplot,
                   row.names = 1:length(freqplot))[1:nwords,]
  ggplot(wf, aes(term, occurrences)) +
    geom_bar(stat="identity") +
    coord_flip() +
    scale_x_discrete(limits=wf$term)
}

# Plot occurrences of single words, 2-grams, and 3-grams
plot_occurrences(freqWords) + ggtitle("Occurrences of top single words")
plot_occurrences(freqWords_blogs) + ggtitle("Occurrences of top single words - Blogs")
plot_occurrences(freqWords_news) + ggtitle("Occurrences of top single words - News")
plot_occurrences(freqWords_twitter) + ggtitle("Occurrences of top single words - Twitter")
plot_occurrences(freqBigram) + ggtitle("Occurrences of top 2-grams")
plot_occurrences(freqTrigram,30) + ggtitle("Occurrences of top 3-grams")

# Unique words required for 50% and 90% of all words
nwords <- NULL
percentcover <- NULL
for (i in 1:length(freqWords)){
  nwords = c(nwords, i)
  percentcover <- c(percentcover, sum(freqWords[1:i])/sum(freqWords)*100)
}
plot(x = nwords, y = percentcover,
     main = "Percent coverage of words",
     xlab = "Number of words",
     ylab = "Percent coverage (%)")
nwords.5 = nwords[which(percentcover>50)[1]]
nwords.9 = nwords[which(percentcover>90)[1]]

# Determine if word is in the English dictionary
indictionary <- hunspell_check(names(freqWords))

# Estimate how many lines were in English
lang <- detect_language(df$lines)
num_en <- table(lang)["en"]

# Extract stems from words
stem_doc <- stemDocument(names(freqWords))
stems <- unique(stem_doc)
```

Based on the results above we observe that:

- There a total of **`r length(freqWords)`** unique words in the subset of this corpora
- It's also interesting that the top word for news articles is **`r names(freqWords_news)[1]`** and the top 2 words for tweets are **`r names(freqWords_twitter)[1]`** and **`r names(freqWords_twitter)[2]`**
- The top **`r nwords.5`** unique words cover 50% of all word instances and the top **`r nwords.9`** unique words cover 90% of all word instances
- **`r sum(indictionary == F)`** of the words were not in the English dictionary
- In addition, the cld3 package was used to evaluate each line from the dataset. **`r num_en`** of the **`r length(df$lines)`** total lines were detected to be English. However, the accuracy of the cld3 package functions would require further validation.
- By stemming, the number of unique words can be reduced down to **`r length(stems)`** stem words

# n-gram model development

```{r predict-model-funcs}
# define function that splits sentence
getSplitSent <- function(sen, nwrds){
  wrds <- unlist(strsplit(sen, split = " "))
  lastNwrds <- paste(tail(wrds,nwrds), collapse = " ")
  remaining <- paste(head(wrds,length(wrds)-nwrds), collapse = " ")
  c(remaining, lastNwrds)
}

getProbMatrix <- function(inputDocs, maxngram = 3, bareMatOnly = F, sparsity = 1.0) {
  #TODO: add "sparse" option to reduce matrix size
  # Clean corpus (do not remove stopwords) and tokenize
  cleanDocs <- Clean_Corpus(inputDocs, removeStpwrds = F)
  freqList <- list()
  freqList[[1]] <- maxngram
  # Convert frequency vectors into data tables and split words in ngram cols
  for (num in 2:maxngram){
    # Remove nonenglish words from freq vectors
    engFreqs <- getEngFreq(Get_Freq(Get_dtm(cleanDocs, n = num, spars = sparsity)))
    dt <- data.table(term = names(engFreqs), freq = engFreqs)
    dt[,remainingTerm := getSplitSent(term, 1)[1], by= 1:nrow(dt)]
    dt[,lastWrd := getSplitSent(term, 1)[2], by= 1:nrow(dt)]
    dt[,rTermFreq := sum(freq), by = .(remainingTerm)]
    # Calculate probabilities and logs
    dt[,p := freq / rTermFreq]
    dt[,logp := log(p)]
    if (bareMatOnly == T) dt[,c("term", "freq", "rTermFreq", "p"):=NULL]
    freqList[[num]] <- dt
  }
  return(freqList)
}

predictNxtWrd <- function(inputpmat, inputsent) {
  # determine starting n based on length of input sentence and maxngram in prob matrix
  numWrds <- length(strsplit(inputsent, split = " ")[[1]])
  maxngram <- inputpmat[[1]]
  n <- numWrds + 1
  if (n > maxngram) n <- maxngram
  # Use "Backoff" to determine probabilities
  #TODO: develop an actual "stupid backoff" model uses 0.4 x probability of next ngram down to calculate probabilities
  for (i in n:2){
    lastNWrds <- getSplitSent(inputsent, i - 1)[2]
    # format input sentence
    tempdoc <- VCorpus(VectorSource(lastNWrds),
                       readerControl = list(reader=readPlain, language = "en"))
    tempdoc <- Clean_Corpus(tempdoc, removeStpwrds = F)
    lastNWrdsFormatted <- tempdoc[[1]]$content
    # select corresponding ngram matrix
    pdt <- inputpmat[[i]]
    subpdt <- pdt[remainingTerm == lastNWrdsFormatted]
    # if no match  for last-n-words then use next ngram down, otherwise break loop
    if (nrow(subpdt)!=0) break
  }
  
  #TODO: optimize so whole list doesn't have to be ordered just to get top words
  setorderv(subpdt, c("logp"), c(-1))
  predictTop <- subpdt$lastWrd[1:5]
}
```


```{r model-testing}
pmat_sp <- getProbMatrix(docsOrig, bareMatOnly = T, sparsity = .6)
format(object.size(pmat_sp), units = "auto")
print(predictNxtWrd(pmat_sp, "hello there"))
```
# Other code (for quizes)

```{r}
dftwitter <- df[which(df$file=="en_US.twitter.txt"),]
dt = dftwitter$lines
sum(grepl(pattern = "love", x = dt))
```