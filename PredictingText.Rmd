---
title: "PredictingText"
author: "AM"
date: "5/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(readr)
library(tm)
```

# Data Processing

```{r data-import}
# Set directories for files
setwd("C:/Users/amanafpour/Desktop/final/en_US")

# Load the dataset
filenames <- c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")
df <- data.frame(file = character(), lines = character())
for (filename in filenames) {
  lins <- readr::read_lines(filename)
  newdf <- data.frame(file = filename, lines = lins)
  df <- rbind(df, newdf)
}
df$file<-as.factor(df$file)
```

# Exploratory Data Analysis

```{r data-analysis}
df <- df[1:10000,] # Use subset of data initially

docs <- VCorpus(VectorSource(df$lines))
dtm <- DocumentTermMatrix(docs)
freq <- colSums(as.matrix(dtm))
ord <- order(freq, decreasing = T)
barplot(freq[ord][1:500], main = "Frequency of top 500 words (trucated graph)", ylab = "Frequency", ylim = c(0,2000))
freqpercent <- head(table(freq)/length(freq), 10) * 100
barplot(freqpercent,
        main = "Histogram of Frequencies",
        xlab = "Number of times a word is used",
        ylab = "Percent of words in dictionary")

# Evaluate 2-grams and 3-grams
BigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)

TrigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)

dtmBigram <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtmTrigram <- DocumentTermMatrix(docs, control = list(tokenize = TrigramTokenizer))
freqBigram <- colSums(as.matrix(dtmBigram))
freqTrigram <- colSums(as.matrix(dtmTrigram))
ordBigram <- order(freqBigram, decreasing = T)
ordTrigram <- order(freqTrigram, decreasing = T)
```

Based on the results above we can conclude:

- There a total of `r length(freq)` different words in the dictionary for this data set
- The majority of words (`r round(freqpercent[1],1)`%) are used only once 
- Based on the top words barpllot above, there is a share drop in frequency

# Other code (for quizes)
```{r}
dftwitter <- df[which(df$file=="en_US.twitter.txt"),]
dt = dftwitter$lines
sum(grepl(pattern = "love", x = dt))
```