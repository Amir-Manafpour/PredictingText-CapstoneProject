---
title: "PredictingText"
author: "AM"
date: "5/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(readr)
library(tm)
library(dplyr)
library(hunspell)
library(ggplot2)
memory.limit(100000)
```

# Data Import

```{r data-import}
# Set directories for files
setwd("C:/Users/amanafpour/Desktop/final/en_US")

# Load the dataset
filenames <- c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")
dfread <- data.frame(file = character(), lines = character())
linesnmax <- 50000 #Set maximum for number of lines read of each file
for (filename in filenames) {
  lins <- readr::read_lines(filename, n_max = linesnmax) 
  newdf <- data.frame(file = filename, lines = lins)
  dfread <- rbind(df, data.frame(file = filename, lines = lins))
}
dfread$file<-as.factor(dfread$file)

# Randomly select small subset of data
nsamp <- 2000
set.seed(34341)
dftibble <- dfread %>% group_by(file) %>% sample_n(size = nsamp)
df <- as.data.frame(dftibble)
```

It is worth noting, for testing purposes and RAM limitations, only a small subset (**`r nsamp`** samples per input file) was selected with equal amounts of samples for each input file. Also, only the first **`r format(linesnmax, scientific = F)`** lines were read from each input file.

# Data Cleaning

```{r data-cleaning}
Clean_Line <- function(string){
    # Lowercase
    temp <- tolower(string)
    # Remove everything that is not a letter or an apostrophe (')
    temp <- gsub(pattern = "[^a-zA-Z\\s|^']", replacement = " ", x = temp)
    # Remove extra whitespace
    temp <- gsub(pattern = "\\s+", replacement = " ", x = temp)
    # Remove explicitly profane words
    profanity <- readLines("http://www.bannedwordlist.com/lists/swearWords.txt")
    temp <- removeWords(temp, profanity)
}
df$lines <- Clean_Line(df$lines)
```

# Tokenization

```{r tokenization}
# Tokenization of single words
docs <- VCorpus(VectorSource(df$lines))
dtm <- DocumentTermMatrix(docs)
freq <- colSums(as.matrix(dtm))
ord <- order(freq, decreasing = T)
freq <- freq[ord]

# Tokenization of 2-grams and 3-grams
BigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
TrigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
dtmBigram <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtmTrigram <- DocumentTermMatrix(docs, control = list(tokenize = TrigramTokenizer))
freqBigram <- colSums(as.matrix(dtmBigram))
freqTrigram <- colSums(as.matrix(dtmTrigram))
ordBigram <- order(freqBigram, decreasing = T)
ordTrigram <- order(freqTrigram, decreasing = T)
freqBigram <- freqBigram[ordBigram]
freqTrigram <- freqTrigram[ordTrigram]
```

# Exploratory Data Analysis

```{r exploratory-analysis}
# Function for plotting terms vs occurrences
plot_occurrences <- function(freqplot){
  wf <- data.frame(term = names(freqplot), occurrences = freqplot, row.names = 1:length(freqplot))[1:20,]
  ggplot(wf, aes(term, occurrences)) +
    geom_bar(stat="identity") +
    coord_flip() +
    scale_x_discrete(limits=wf$term)
}

# Plot occurences of single words, 2-grams, and 3-grams
plot_occurrences(freq) + ggtitle("Occurrences of top single words (truncated graph)")
plot_occurrences(freqBigram) + ggtitle("Occurrences of top 2-grams")
plot_occurrences(freqTrigram) + ggtitle("Occurrences of top 3-grams")

# Unique words required for 50% and 90% of all words
nwords <- NULL
percentcover <- NULL
for (i in 1:length(freq)){
  nwords = c(nwords, i)
  percentcover <- c(percentcover, sum(freq[1:i])/sum(freq)*100)
}
plot(x = nwords, y = percentcover,
     main = "Percent coverage of words",
     xlab = "Number of words",
     ylab = "Percent coverage (%)")
nwords.5 = nwords[which(percentcover>.5)[1]]
nwords.9 = nwords[which(percentcover>.9)[1]]

# Determine if word is in the english dictionary
indictionary <- hunspell_check(names(freq))

# Estimate how many lines were in english
lang <- detect_language(df$lines)
num_en <- sum(lang=="en")

# Extract stems from words
stem_doc <- stemDocument(names(freq))
stems <- unique(stem_doc)
```

Based on the results above we observe that:

- There a total of **`r length(freq)`** unique words in the subset of this corpora
- The majority of words (**`r round(freqpercent[1],1)`**%) are used only once 
- Based on the frequency plots above, the word frequencies appear to have a poisson distribution
- The top **`r nwords.5`** unique words cover 50% of all word instances and the top **`r nwords.9`** unique words cover 90% of all word instances
- **`r sum(indictionary == F)`** of the words were not in the English dictionary
- In addition, the cld3 package was used to evaluate each line from the dataset. **`r num_en`** of the **`r length(df$lines)`** total lines were detected to be English. However, the accuracy of the cld3 package functions would require further validation.
- By stemming, the number of words can be reduced down to **`r length(stems)`** stem words

# Other code (for quizes)

```{r}
dftwitter <- df[which(df$file=="en_US.twitter.txt"),]
dt = dftwitter$lines
sum(grepl(pattern = "love", x = dt))
```